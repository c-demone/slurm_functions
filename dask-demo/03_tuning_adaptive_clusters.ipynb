{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Monte-Carlo Estimate of $\\pi$\n",
    "\n",
    "We want to estimate the number $\\pi$ using a [Monte-Carlo method](https://en.wikipedia.org/wiki/Pi#Monte_Carlo_methods) exploiting that the area of a quarter circle of unit radius is $\\pi/4$ and that hence the probability of any randomly chosen point in a unit square to lie in a unit circle centerd at a corner of the unit square is $\\pi/4$ as well.  So for N randomly chosen pairs $(x, y)$ with $x\\in[0, 1)$ and $y\\in[0, 1)$, we count the number $N_{circ}$ of pairs that also satisfy $(x^2 + y^2) < 1$ and estimage $\\pi \\approx 4 \\cdot N_{circ} / N$.\n",
    "\n",
    "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/84/Pi_30K.gif\" \n",
    "     width=\"50%\" \n",
    "     align=top\n",
    "     alt=\"PI monte-carlo estimate\">](https://en.wikipedia.org/wiki/Pi#Monte_Carlo_methods)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Core Lessons\n",
    "\n",
    "- Recap adaptive clusters\n",
    "- Tuning the adaptivity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup for Example"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from os.path import exists\n",
    "from pathlib import Path\n",
    "\n",
    "home = str(Path.home())\n",
    "dasklogs = f\"{home}/dask-demo-logs\"\n",
    "if not exists(dasklogs):\n",
    "    os.mkdir(dasklogs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up a Slurm cluster"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import os\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    cores=24,\n",
    "    processes=2,\n",
    "    memory=\"100GB\",\n",
    "    shebang='#!/usr/bin/env bash',\n",
    "    queue=\"normal\",\n",
    "    walltime=\"00:30:00\",\n",
    "    local_directory='/tmp',\n",
    "    death_timeout=\"15s\",\n",
    "    interface=\"ib0\",\n",
    "    log_directory=dasklogs,\n",
    "    project=\"boc\")\n",
    "\n",
    "client = Client(cluster)\n",
    "client"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The job scripts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(cluster.job_script())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scale the cluster to two nodes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cluster.scale(4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Monte Carlo Method"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_pi_mc(size_in_bytes, chunksize_in_bytes=200e6):\n",
    "    \"\"\"Calculate PI using a Monte Carlo estimate.\"\"\"\n",
    "    \n",
    "    size = int(size_in_bytes / 8)\n",
    "    chunksize = int(chunksize_in_bytes / 8)\n",
    "    \n",
    "    xy = da.random.uniform(0, 1,\n",
    "                           size=(size / 2, 2),\n",
    "                           chunks=(chunksize / 2, 2))\n",
    "    \n",
    "    in_circle = ((xy ** 2).sum(axis=-1) < 1)\n",
    "    pi = 4 * in_circle.mean()\n",
    "\n",
    "    return pi\n",
    "\n",
    "\n",
    "def print_pi_stats(size, pi, time_delta, num_workers):\n",
    "    \"\"\"Print pi, calculate offset from true value, and print some stats.\"\"\"\n",
    "    print(f\"{size / 1e9} GB\\n\"\n",
    "          f\"\\tMC pi: {pi : 13.11f}\"\n",
    "          f\"\\tErr: {abs(pi - np.pi) : 10.3e}\\n\"\n",
    "          f\"\\tWorkers: {num_workers}\"\n",
    "          f\"\\t\\tTime: {time_delta : 7.3f}s\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The actual calculations\n",
    "\n",
    "We loop over different volumes of double-precision random numbers and estimate $\\pi$ as described above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from time import time, sleep"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for size in (1e9 * n for n in (1, 10, 100)):\n",
    "    \n",
    "    start = time()\n",
    "    pi = calc_pi_mc(size).compute()\n",
    "    elaps = time() - start\n",
    "\n",
    "    print_pi_stats(size, pi, time_delta=elaps,\n",
    "                   num_workers=len(cluster.scheduler.workers))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scaling the Cluster to twice its size\n",
    "\n",
    "We increase the number of workers by 2 and the re-run the experiments."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "new_num_workers = 2 * len(cluster.scheduler.workers)\n",
    "\n",
    "print(f\"Scaling from {len(cluster.scheduler.workers)} to {new_num_workers} workers.\")\n",
    "\n",
    "cluster.scale(new_num_workers)\n",
    "\n",
    "sleep(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Re-run same experiments with doubled cluster"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for size in (1e9 * n for n in (1, 10, 100)):\n",
    "    \n",
    "    start = time()\n",
    "    pi = calc_pi_mc(size).compute()\n",
    "    elaps = time() - start\n",
    "\n",
    "    print_pi_stats(size, pi,\n",
    "                   time_delta=elaps,\n",
    "                   num_workers=len(cluster.scheduler.workers))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Automatically scale the cluster towards a target duration\n",
    "\n",
    "Previously, we have seen how to let Dask figure out the optimal cluster size.  Here, we'll target a wall time of 30 seconds.\n",
    "\n",
    "_**Watch** how the cluster will scale down to the minimum a few seconds after being made adaptive._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ca = cluster.adapt(\n",
    "    minimum=2, maximum=30,\n",
    "    target_duration=\"360s\",  # measured in CPU time per worker\n",
    "                             # -> 30 seconds at 12 cores / worker\n",
    "    scale_factor=1.0  # prevent from scaling up because of CPU or MEM need\n",
    ");\n",
    "\n",
    "sleep(10)  # Allow for scale-down"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Repeat the calculation from above with larger work loads\n",
    "\n",
    "(And watch the dash board!)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for size in (n * 1e9 for n in (200, 400, 800)):\n",
    "    \n",
    "    \n",
    "    start = time()\n",
    "    pi = calc_pi_mc(size, min(size / 1000, 500e6)).compute()\n",
    "    elaps = time() - start\n",
    "\n",
    "    print_pi_stats(size, pi, time_delta=elaps,\n",
    "                   num_workers=len(cluster.scheduler.workers))\n",
    "    \n",
    "    sleep(20)  # allow for scale-down time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Complete listing of software used here"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%pip list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%conda list --explicit"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dask_jobqueue_workshop]",
   "language": "python",
   "name": "conda-env-dask_jobqueue_workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}